import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import cv2
import numpy as np
from mediapipe.python.solutions.holistic import Holistic
from helpers import create_folder, draw_keypoints, mediapipe_detection, save_frames, there_hand
from constants import FONT, FONT_POS, FONT_SIZE, FRAME_ACTIONS_PATH, ROOT_PATH

def capture_samples(path, margin_frame=2, min_cant_frames=5):
    '''
       ### CAPTURA DE MUESTRAS PARA UNA PALABRA
       Recibe como parámetro la  ubicación de guardado y guarda los frames

       `path` ruta de la carpeta de la palabra \n
       `margin_frame` cantidad de frames que se ignoran al comienzo y al final \n
       `min_cant_frames` cantidad de frames minimos para cada muestra
    '''

    create_folder(path)

    cant_samples_exist = len(os.listdir(path))
    count_sample = 0
    count_frame = 0
    frames = []

    with Holistic() as holistic_model:
        video = cv2.VideoCapture(0)
        video.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        video.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

        while video.isOpened():
            frame = video.read()[1]
            image, results = mediapipe_detection(frame, holistic_model)

            if there_hand(results):
                count_frame += 1
                if count_frame > margin_frame:
                    cv2.putText(image, 'Capturando...', FONT_POS, FONT, FONT_SIZE, (255, 50, 0))
                    frames.append(np.asarray(frame))

            else:
                if len(frames) > min_cant_frames + margin_frame:
                    frames = frames[:-margin_frame]
                    output_folder = os.path.join(path, f"sample_{cant_samples_exist + count_sample +1}")
                    create_folder(output_folder)
                    save_frames(frames, output_folder)
                    count_sample += 1

                frames = []
                count_frame = 0
                cv2.putText(image, 'Listo para capturar....', FONT_POS, FONT, FONT_SIZE, (0, 220, 100))

            draw_keypoints(image, results)
            cv2.imshow(f'Toma de muestras para "{os.path.basename(path)}"', image)

            if cv2.waitKey(10) & 0xFF == ord('q'):
                break

        video.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
        word_name = "Buenos_dias"
        word_path = os.path.join(ROOT_PATH, FRAME_ACTIONS_PATH, word_name)
        capture_samples(word_path)

Evaluate_model.py

import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import cv2
import numpy as np
from mediapipe.python.solutions.holistic import Holistic
from keras.models import load_model
from helpers import draw_keypoints, extract_keypoints, format_sentences, get_actions, mediapipe_detection, save_txt, there_hand
from text_to_speech import text_to_speech
from constants import DATA_PATH, FONT, FONT_POS, FONT_SIZE, MAX_LENGTH_FRAMES, MIN_LENGHT_FRAMES, MODELS_PATH, MODEL_NAME, ROOT_PATH

def evaluate_model(model, threshold=0.7):
    count_frame = 0
    repe_sent = 1
    kp_sequence, sentence = [], []
    actions = get_actions(DATA_PATH)

    with Holistic() as holistic_model:
        video = cv2.VideoCapture(0)

        while video.isOpened():
            _, frame = video.read()

            image, results = mediapipe_detection(frame, holistic_model)
            kp_sequence.append(extract_keypoints(results))

            if len(kp_sequence) > MAX_LENGTH_FRAMES and there_hand(results):
                count_frame += 1

            else:
                if count_frame >= MIN_LENGHT_FRAMES:
                    res = model.predict(np.expand_dims(kp_sequence[-MAX_LENGTH_FRAMES:], axis=0))[0]

                    if res[np.argmax(res)] > threshold:
                        sent = actions[np.argmax(res)]
                        sentence.insert(0, sent)
                        text_to_speech(sent)
                        sentence, repe_sent = format_sentences(sent, sentence, repe_sent)

                    count_frame = 0
                    kp_sequence = []

            cv2.rectangle(image, (0,0), (640, 35), (245, 117, 16), -1)
            cv2.putText(image, ' | '.join(sentence), FONT_POS, FONT, FONT_SIZE, (255, 255, 255))
            save_txt('outputs/sentences.txt', '\n'.join(sentence))

            draw_keypoints(image, results)
            cv2.imshow('Traductor LSP', image)
            if cv2.waitKey(10) & 0xFF == ord('q'):
                break

        video.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    model_path = os.path.join(MODELS_PATH, MODEL_NAME)
    lstm_model = load_model(model_path)
    evaluate_model(lstm_model)

    Capture_samples desde video de youtube

    import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import cv2
import numpy as np
from mediapipe.python.solutions.holistic import Holistic
from helpers import create_folder, draw_keypoints, mediapipe_detection, save_frames, there_hand
from constants import FONT, FONT_POS, FONT_SIZE, FRAME_ACTIONS_PATH, ROOT_PATH
from pytube import YouTube

def download_youtube_video(youtube_url, output_path):
    yt = YouTube(youtube_url)
    yt.streams.filter(progressive=True, file_extension='mp4').first().download(output_path)

def capture_hand_frames_from_youtube(youtube_url, output_folder):
    create_folder(output_folder)
    print(f'Carpeta de salida creada en: {output_folder}')  # Mensaje de depuración

    # Descargar el video de YouTube
    download_youtube_video(youtube_url, output_folder)
    print("Video descargado exitosamente")

    video_filename = os.path.join(output_folder, os.listdir(output_folder)[0])

    print("Comenzando la captura de frames con manos detectadas...")  # Mensaje de depuración

    with Holistic() as holistic_model:
        cap = cv2.VideoCapture(video_filename)
        if not cap.isOpened():
            print("Error: No se pudo abrir el video descargado")
            return

        frame_count = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            image, results = mediapipe_detection(frame, holistic_model)

            if there_hand(results):
                frame_filename = os.path.join(output_folder, f"hand_frame_{frame_count}.jpg")
                cv2.imwrite(frame_filename, frame)
                print(f"Frame con manos guardado en: {frame_filename}")  # Mensaje de depuración
                frame_count += 1

        cap.release()
        cv2.destroyAllWindows()

        # Eliminar el archivo de video después de la captura de frames
        os.remove(video_filename)
        print("Archivo de video eliminado")  # Mensaje de depuración

    print("Fin de la captura de frames con manos detectadas")  # Mensaje de depuración

if __name__ == "__main__":
    youtube_url = input("Por favor, introduce la URL del video de YouTube: ")
    output_folder_name = input("Por favor, introduce el nombre de la carpeta para guardar los frames: ")
    output_folder = os.path.join(ROOT_PATH, FRAME_ACTIONS_PATH, output_folder_name)
    capture_hand_frames_from_youtube(youtube_url, output_folder)

html
<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mi Página Web</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.1.3/socket.io.js" integrity="sha512-3+8/DL4vHswWvWvrgKRWJzq5ph3zqjr4ZW3rxdlwRjRmVEMWLn9iWvT3mN+w9iLVkLc3WQDZvMS02oMaKbqpuQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="stylesheet" href="../static/style.css">
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
        <div class="container">
            <a class="navbar-brand" href="#">Mi Página</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item active">
                        <a class="nav-link" href="#">Inicio</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#">Acerca de</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#">Servicios</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#">Contacto</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <div class="jumbotron jumbotron-fluid" id="jumbotron">
        <div class="container">
            <div class="row">
                <div class="col-lg-6 left-content">
                    <h1 class="display-4">Bienvenido a mi página web</h1>
                    <p class="lead">Aquí puedes encontrar información interesante.</p>
                </div>
                <div class="col-lg-6 right-content">
                    <div class="d-flex flex-column align-items-center justify-content-start camera-container">
                        <div class="camera-box"></div>
                        <div class="text-center mt-3">
                            <button id="start-btn">Iniciar Procesamiento del Modelo</button>

                            <script>
                                var socket = io();

                                document.getElementById('start-btn').addEventListener('click', function() {
                                    socket.emit('start_processing');
                                 });
                            </script>
                            <h1 class="display-4">Bienvenido a mi página web</h1>
                            <p class="lead">Aquí puedes encontrar información interesante.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <footer class="footer fixed-bottom">
        <div class="container">
            <p>Derechos Reservados &copy; 2024</p>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.3/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <script src="../static/script.js"></script>
</body>
</html>

.js
document.addEventListener("DOMContentLoaded", function() {
    const images = [
        "../static/img/pexels-cottonbro-studio-6332434.jpg",
        "../static/img/pexels-cottonbro-studio-6332432.jpg.",
        "../static/img/pexels-cottonbro-studio-6322489.jpg",
        "../static/img/pexels-cottonbro-studio-6321941.jpg",
        "../static/img/pexels-cottonbro-studio-6321925.jpg"
    ];

    let currentIndex = 0;
    const jumbotron = document.getElementById('jumbotron');

    function changeBackground() {
        jumbotron.style.backgroundImage = `url('${images[currentIndex]}')`;
        currentIndex = (currentIndex + 1) % images.length;
    }

    // Cambiar la imagen de fondo cada 5 segundos (5000 milisegundos)
    setInterval(changeBackground, 5000);
});

evaluate model final

import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import cv2
import numpy as np
from mediapipe.python.solutions.holistic import Holistic
from keras.models import load_model
from helpers import draw_keypoints, extract_keypoints, format_sentences, get_actions, mediapipe_detection, save_txt, there_hand
from text_to_speech import text_to_speech
from constants import DATA_PATH, FONT, FONT_POS, FONT_SIZE, MAX_LENGTH_FRAMES, MIN_LENGHT_FRAMES, MODELS_PATH, MODEL_NAME, ROOT_PATH
import threading

def evaluate_model(model, threshold=0.7):
    count_frame = 0
    repe_sent = 1
    kp_sequence, sentence = [], []
    actions = get_actions(DATA_PATH)

    with Holistic() as holistic_model:
        video = cv2.VideoCapture(0)

        while video.isOpened():
            _, frame = video.read()

            image, results = mediapipe_detection(frame, holistic_model)
            kp_sequence.append(extract_keypoints(results))

            if len(kp_sequence) > MAX_LENGTH_FRAMES and there_hand(results):
                count_frame += 1

            else:
                if count_frame >= MIN_LENGHT_FRAMES:
                    res = model.predict(np.expand_dims(kp_sequence[-MAX_LENGTH_FRAMES:], axis=0))[0]

                    if res[np.argmax(res)] > threshold:
                        sent = actions[np.argmax(res)]
                        sentence.insert(0, sent)
                        threading.Thread(target=text_to_speech, args=(sent,)).start()
                        sentence, repe_sent = format_sentences(sent, sentence, repe_sent)

                    count_frame = 0

                    count_frame = 0
                    kp_sequence = []

            cv2.rectangle(image, (0,0), (640, 35), (245, 117, 16), -1)
            cv2.putText(image, ' | '.join(sentence), FONT_POS, FONT, FONT_SIZE, (255, 255, 255))
            save_txt('outputs/sentences.txt', '\n'.join(sentence))

            draw_keypoints(image, results)
            cv2.imshow('Traductor LSP', image)
            if cv2.waitKey(10) & 0xFF == ord('q'):
                break

        video.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    model_path = os.path.join(MODELS_PATH, MODEL_NAME)
    lstm_model = load_model(model_path)
    evaluate_model(lstm_model)
